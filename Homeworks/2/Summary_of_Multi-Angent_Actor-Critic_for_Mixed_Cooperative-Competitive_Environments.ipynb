{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3460b072-f14a-463c-8d95-875f45a9cf6a",
   "metadata": {},
   "source": [
    "# Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb6fbfd-b919-49c7-abbd-23f2637356f6",
   "metadata": {},
   "source": [
    "### 1. Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e8963-ed0e-4aab-9442-f47e47e076bf",
   "metadata": {},
   "source": [
    "The authors investigate deep reinforcement learning methods for multi-agent domains. They begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by the inherent non-stationarity of the environment, while the policy gradient suffers from a variance that increases as the number of agents increases. The authors present an adaptation of actor-critic methods that takes into account the action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. They also introduce a training mode using a set of policies for each agent, leading to more robust multi-agent policies. They show the power of their approach compared to existing methods in cooperative as well as competitive scenarios where agent populations can discover different physical and informational coordination strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f8bff9-c927-44eb-b756-23d9accf10c7",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Abstract](#1.-Abstract)\n",
    "2. [Introduction](#2.-Introduction)\n",
    "3. [Related Work](#3.-Related-Work)    \n",
    "4. [Background](#4.-Background)\n",
    "5. [Methods](#5.-Methods)\n",
    "    - [5.1. Multi-Agent Actor Critic](#5.1.-Multi-Agent-Actor-Critic)\n",
    "    - [5.2. Inferring Policies of Other Agents](#5.2.-Inferring-Policies-of-Other-Agents)\n",
    "    - [5.3. Agents with Policy Ensembles](#5.3.-Agents-with-Policy-Ensembles)\n",
    "6. [Experiments](#6.-Experiments)\n",
    "    - [6.1. Environments](#6.1.-Environments)\n",
    "    - [6.2. Comparison to Decentralized Reinforcement Learning Methods](#6.2.-Comparison-to-Decentralized-Reinforcement-Learning-Methods)\n",
    "    - [6.3. Effect of Learning Polices of Other Agents](#6.3.-Effect-of-Learning-Polices-of-Other-Agents)\n",
    "    - [6.4. Effect of Training with Policy Ensembles](#6.4.-Effect-of-Training-with-Policy-Ensembles)\n",
    "7. [Conclusions and Future Work](#7.-Conclusions-and-Future-Work)\n",
    "8. [Resources](#8.-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c8e9a0-6d01-41bf-a533-653e075eac54",
   "metadata": {},
   "source": [
    "### 2. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed90c8-0666-4aa1-affe-9af070f8b8e2",
   "metadata": {},
   "source": [
    "Reinforcement learning (RL) has recently been applied to solve challenging problems, from game playing to robotics. In industrial applications, RL is emerging as a practical component in large scale systems such as data center cooling. Most of the successes of RL have been in single agent domains, where modelling or predicting the behaviour of other actors in the environment is largely unnecessary.\n",
    "\n",
    "However, there are a number of important applications that involve interaction between multiple agents, where emergent behavior and complexity arise from agents co-evolving together. For example, multi-robot control, the discovery of communication and language, multiplayer games, and the analysis of social dilemmas all operate in a multi-agent domain. Related problems, such as variants of hierarchical reinforcement learning can also be seen as a multi-agent system, with multiple levels of hierarchy being equivalent to multiple agents. Additionally, multi-agent self-play has recently been shown to be a useful training paradigm. Successfully scaling RL to environments with multiple agents is crucial to building artificially intelligent systems that can productively interact with humans and each other.\n",
    "\n",
    "Unfortunately, traditional reinforcement learning approaches such as Q-Learning or policy gradient are not suitable for multi-agent environments. One problem, according to the authors, is that each agent's policy changes as learning progresses and the environment becomes non-stationary from everyone's perspective individual agent (in a way that cannot be explained by changes in the agent's own policy). This presents stability of learning is challenging and prevents the direct use of repetition of past experience, which is critical for stabilizing deep Q-learning. Policy gradient methods, on the other hand, usually show very large variance when coordination of multiple agents is required. Alternatively, model-based policy optimization can be used, which can learn optimal policies by back-propagation, but this requires (differentiable) model of world dynamics and assumptions about the interactions between agents. Applying these methods in competitive environments is also an optimization challenge perspective, as evidenced by the notorious instability of competitive learning methods.\n",
    "\n",
    "In this work, the authors of the paper propose a general-purpose multi-agent learning algorithm that: (1) results in a learned policies that use only local information (ie, their own observations) at runtime, (2) does does not assume a differentiable model of the dynamics of the environment or any particular structure of method of communication between agents and (3) is applicable not only to cooperative interaction but to competitive or mixed interaction involving both physical and communicative behavior. The the ability to operate in mixed cooperative-competitive environments may be critical for intelligent agents; while adversarial learning provides a natural curriculum for learning, agents must also perform cooperative behavior (eg with humans) during performance.\n",
    "\n",
    "They adopt the framework of centralized learning with decentralized implementation, which allows policies to use additional information to facilitate learning, as long as that information is not used during the test. it is unnatural to do this with Q-training without making additional assumptions about the structure of the environment, since the Q-function cannot usually contain different information at training and test time. Thus, the authors propose a simple extension of actor-critic policy gradient methods, where the critic is supplemented with additional information about the policies of other agents, while the actor only has access to local information. After training is completed, only local actors are used in the implementation phase, acting in a decentralized manner and equally applicable in cooperative and competitive environments.\n",
    "\n",
    "Since the centralized critical function explicitly uses the decision policies of other agents, the authors further show that agents can learn approximate models of other agents online and use them effectively in their own policy training procedure. The authors also introduce a method to improve the stability of multi-agent policies by training agents with an ensemble of policies, thus requiring robust interaction with different policies for collaborators and competitors. Empirically show the success of their approach compared to existing methods in cooperative as well as competitive scenarios where an agent populations are able to discover complex physical and communicative strategies for coordination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab3c97-144c-4d5a-99b2-8771332ffe26",
   "metadata": {},
   "source": [
    "### 3. Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde548f7-6f1e-45da-aab6-9ec95bdcfcf6",
   "metadata": {},
   "source": [
    "The simplest approach to learning in multi-agent settings is to use independent learning agents. It was done with Q-learning in, but did not perform well in practice. As we wish show that gradient methods of self-learning policies also perform poorly. One question is that each the agent's policy changes during training, which leads to a non-stationary environment and prevents naive application of trial repetition. Previous work has attempted to address this through induction other agent policy parameters to the function Q, explicitly adding the iteration index to replay buffer or use importance sampling. Deep Q-learning approaches have existed before investigated in for training competitive Pong agents.\n",
    "\n",
    "Simultaneously with the authors' work proposed a similar idea of using policy gradient methods with a centralized critic and are test their approach on a StarCraft micromanagement task. Their (StarCraft) approach differs from the authors in the following ways: (1) they (StarCraft) learn a centralized critic for all agents, while the authors learn a centralized critique for each agent, allowing agents with different reward functions including competitive scenarios, (2) the authors also consider environments with explicit communication between agents, (3) they combine iterative policies with prior critics, while author experiments use feed-forward policies (although their methods are applicable to recurrent policies), (4) learn continuous policies, while they learn discrete policies.\n",
    "\n",
    "Recent work has focused on learning grounded cooperative communication protocols between agents to solve various tasks. However, these methods are usually only applicable when the communication between agents is carried out over a dedicated, differentiable communication channel.\n",
    "\n",
    "Their method requires explicit modeling of the decision-making process by other agents. The importance such modeling is recognized by both reinforcement learning and cognitive science communities emphasized the importance of being stable in the decision-making process other agents, as well as others by building Bayesian decision-making models. The authors of the article include such resiliency considerations, requiring agents to interact successfully with an ensemble of any possible policies of other agents, improving the robustness of training and the robustness of agents after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e335103-80c6-4ad1-a410-9cd272d825db",
   "metadata": {},
   "source": [
    "### 4. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d38d7-a12f-4997-a9c7-404d560c1842",
   "metadata": {},
   "source": [
    "Markov Games In this work, we consider a multi-agent extension of Markov decision processes\n",
    "(MDPs) called partially observable Markov games. A Markov game for N agents is defined by a\n",
    "set of states S describing the possible configurations of all agents, a set of actions $A_1$, ..., $A_N$ and\n",
    "a set of observations $O_1$, ..., $O_N$ for each agent. To choose actions, each agent i uses a stochastic\n",
    "policy $\\pi_{θ_i}$ : $O_i \\times A_i \\mapsto  $, which produces the next state according to the state transition function\n",
    "$T : S \\times A_1 \\times ... \\times A_N 7 \\mapsto   S^2$. Each agent i obtains rewards as a function of the state and agent’s\n",
    "action $r_i: S \\times A_i\\mapsto   R$, and receives a private observation correlated with the state $o_i : S \\mapsto   O_i$.\n",
    "The initial states are determined by a distribution $ρ : S \\mapsto  $ . Each agent $i$ aims to maximize its\n",
    "own total expected return $\\sum_{t=0}^{T} \\gamma^{t} r_{i}^{t}$ where $\\gamma$ is a discount factor and $T$ is the time horizon.\n",
    "\n",
    "__Q-Learning and Deep Q-Networks (DQN)__. Q-Learning and DQN are popular methods in\n",
    "reinforcement learning and have been previously applied to multi-agent settings. Q-Learning\n",
    "makes use of an action-value function for policy $\\pi$ as $Q^\\pi (s, a) = \\mathbb{E}\\left [R|s^t = s,a^t = a\\right ]$. This $Q$\n",
    "function can be recursively rewritten as $Q^\\pi (s, a) = \\mathbb{E}_{\\acute{s}}\\left[r(s, a) +\\gamma \\mathbb{E}_{\\acute{a}\\sim \\pi} \\left [Q^\\pi(\\acute{s}, \\acute{a})\\right ]\\right ]$. DQN learns the action-value function $Q^∗$ corresponding to the optimal policy by minimizing the loss:\n",
    "\n",
    "$\\mathcal{L}\\left(\\theta\\right) = \\mathbb{E}_{s,a,r,\\acute{s}}\\left[\\left(Q^*\\left(s,a|\\theta \\right)-y\\right)^2\\right]$, where $y = r + \\gamma\\max_{\\acute{a}} \\bar{Q}^*\\left(\\acute{s},\\acute{a} \\right)$,\n",
    "\n",
    "where $\\bar{Q}$  is a target $Q$ function whose parameters are periodically updated with the most recent $\\theta$, which helps stabilize learning. Another crucial component of stabilizing DQN is the use of an experience replay buffer $\\mathcal{D}$ containing tuples $s,a,r,\\acute{s}$.\n",
    "\n",
    "Q-Learning can be directly applied to multi-agent settings by having each agent $i$ learn an independently optimal function $Q_i$. However, because agents are independently updating their policies as learning progresses, the environment appears non-stationary from the view of any one agent, violating Markov assumptions required for convergence of Q-learning. Another difficulty observed in is that the experience replay buffer cannot be used in such a setting since in general,\n",
    "\n",
    "$P\\left(\\acute{s}|s,a,\\pi_1,...,\\pi_N\\right)\\neq P\\left(\\acute{s}|s,a,\\acute{\\pi_1},...,\\acute{\\pi_N}\\right)$ when any $\\pi_i\\neq\\acute{\\pi_i}$.\n",
    "\n",
    "__Policy Gradient (PG) Algorithms.__ Policy gradient methods are another popular choice for a variety of RL tasks. The main idea is to directly adjust the parameters $\\theta$ of the policy in order to maximize the objective $J\\left(\\theta\\right) = \\mathbb{E}_{s\\thicksim p^\\pi,a\\thicksim\\pi_\\theta}\\left[R\\right]$ by taking steps in the direction of $\\nabla_\\theta J\\left(\\theta\\right)$.Using the $Q$ function defined previously, the gradient of the policy can be written as:\n",
    "\n",
    "$\\nabla_\\theta J\\left(\\theta\\right) = \\mathbb{E}_{s\\thicksim p^\\pi,a\\thicksim\\pi_\\theta}\\left[\\nabla _\\theta log\\pi_\\theta\\left(a|s\\right)Q^\\pi\\left(s,a\\right)\\right]$,\n",
    "\n",
    "where $p^\\pi$ is the state distribution. The policy gradient theorem has given rise to several practical algorithms, which often differ in how they estimate $Q^\\pi$. For example, one can simply use a sample return $R^t = \\sum_{i=t}^{T}\\gamma^{i-t}r_i $, which leads to the REINFORCE algorithm. Alternatively, one could learn an approximation of the true action-value function $Q^\\pi\\left(s,a\\right)$ by e.g. temporal-difference learning this $Q^\\pi\\left(s,a\\right)$ is called the critic and leads to a variety of actor-critic algorithms.\n",
    "\n",
    "Policy gradient methods are known to exhibit high variance gradient estimates. This is exacerbated in multi-agent settings; since an agent’s reward usually depends on the actions of many agents, the reward conditioned only on the agent’s own actions (when the actions of other agents are not considered in the agent’s optimization process) exhibits much more variability, thereby increasing the variance of its gradients. Below, we show a simple setting where the probability of taking a gradient step in the correct direction decreases exponentially with the number of agents.\n",
    "\n",
    "__Proposition 1.__ _Consider $N$ agents with binary actions: $P(a_i = 1) = \\theta_i$\n",
    ", where $R(a_1,..., a_N ) = 1_{a_1=· · ·=a_N }$. We assume an uninformed scenario, in which agents are initialized to $\\theta_i = 0.5 \\forall_i$. Then,\n",
    "if we are estimating the gradient of the cost $J$ with policy gradient, we have:_\n",
    "\n",
    "$P\\left(\\left \\langle \\hat{\\nabla}J,\\nabla J \\right \\rangle>0\\right)\\infty\\left(0.5\\right)^N$\n",
    "\n",
    "_where $\\hat{\\nabla}J$ $J$ is the policy gradient estimator from a single sample, and $\\nabla J$ is the true gradient._\n",
    "\n",
    "The use of baselines, such as value function baselines typically used to ameliorate high variance, is problematic in multi-agent settings due to the non-stationarity issues mentioned previously.\n",
    "\n",
    "__Deterministic Policy Gradient (DPG) Algorithms.__ It is also possible to extend the policy gradient framework to deterministic policies $\\mu_\\theta : S \\mapsto A$.In particular, under certain conditions we can write the gradient of the objective $J\\left(\\theta\\right)=\\mathbb{E}_{s\\sim p^\\mu}\\left[R\\left(s,a\\right)\\right]$ as:\n",
    "\n",
    "$\\nabla_\\theta J\\left(\\theta\\right ) = \\mathbb{E}_{s\\sim \\mathcal{D}}\\left[ \\nabla_ \\theta \\mu_\\theta \\left(a|s\\right)\\nabla_a Q^\\mu\\left(s,a\\right)|_{a=\\mu_\\theta\\left(s\\right)}\\right]$\n",
    "\n",
    "Since this theorem relies on $\\nabla_a Q^\\mu\\left(s,a\\right)$, it requires that the action space $A$ (and thus the policy $\\mu$)be continuous.\n",
    "\n",
    "_Deep deterministic policy gradient_ (DDPG) is a variant of DPG where the policy $\\mu$ and critic $Q^\\mu$ are approximated with deep neural networks. DDPG is an off-policy algorithm, and samples trajectories from a replay buffer of experiences that are stored throughout training. DDPG also makes use of a target network, as in DQN .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7316bed1-c42d-4963-8d72-8cfcf6c7cede",
   "metadata": {},
   "source": [
    "### 5. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936d7282-0f1d-43e7-86f7-427dddd52f69",
   "metadata": {},
   "source": [
    "#### 5.1. Multi-Agent Actor Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b1fcd9-be90-4392-8494-9d0a77292cd4",
   "metadata": {},
   "source": [
    "The authors of the previous section argued that naively policy gradient methods perform poorly in simple multi-agent settings and this is supported in the author's experiments in this section. Their purpose in this section is to derive an algorithm that works well in such settings. However, they want to work under the following constraints: (1) learned policies can only use local information (ie, their own observations) at runtime, (2) they do not accept differentiable environment dynamics model, and (3) do not adopt any particular structure the method of communication between agents (i.e. they do not accept differentiable communication channel). Meeting the above requirements would provide general purpose multi-agent training algorithm that can be applied not only to cooperative games with explicit communication channels, but competitive games and games involving only physical interactions between agents.\n",
    "\n",
    "The authors achieved their goal by adopting the framework of centralized learning with decentralized execution. Thus, they allowed policies to use additional information to facilitate learning, so until this information is used during the test. It is unnatural to do this with Q-learning because Q the function cannot normally contain different information during training and test. That's what they offer a simple extension of the actor-critic policy gradient methods where the critic is reinforced with additional information about the policies of other agents.\n",
    "\n",
    "More concretely, consider a game with $N$ agents with policies parameterized by $\\theta = \\{\\theta_1,...,\\theta_N\\}$, and let $\\pi = \\{\\pi_1,...,\\pi_N\\}$be the set of all agent policies. Then we can write the gradient of the expected return for agent $i,J\\left(\\theta_i\\right)=\\mathbb{E}\\left[R_i\\right]$ as:\n",
    "\n",
    "$\\nabla_\\theta J\\left(\\theta_i\\right) = \\mathbb{E}_{s\\thicksim p^\\mu,a_i\\thicksim\\pi_i}\\left[\\nabla _{\\theta_i} log\\pi_i\\left(a_i|o_i\\right)Q_i^\\pi\\left(x,a_1,...,a_N\\right)\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb366c-a1d2-4fe5-923e-2c15cc3d1597",
   "metadata": {},
   "source": [
    "[Source: ](data:image/png;base64,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)![](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc00a2a-35b5-4534-9bb2-3766779cbd96",
   "metadata": {},
   "source": [
    "Here $Q_i^\\pi\\left(x,a_1,...,a_N\\right)$ is a _centralized action-value function_ that takes as input the actions of all agents,$\\left(x,a_1,...,a_N\\right)$, in addition to some state information $x$ could consist of the observations of all agents, $x = \\left(o_1,...,o_N\\right)$, however we could also include additional state information if available. Since each $Q_i^\\pi$ is learned separately, agents can have arbitrary reward structures, including conflicting rewards in a competitive setting. \n",
    "\n",
    "Extend the above idea to work with deterministic policies. If we now consider $N$ continuous\n",
    "policies $\\mu_{\\theta_i}$ w.r.t. parameters $\\theta_i$ (abbreviated as $\\mu_i$), the gradient can be written as:\n",
    "\n",
    "$\\nabla_\\theta J\\left(\\mu_i\\right) = \\mathbb{E}_{x,a \\thicksim \\mathcal{D}}\\left[\\nabla_{\\theta_i}\\mu_i\\left(a_i|o_i\\right)\\nabla_{a_i}Q^\\mu_i\\left(x,a_1,...,a_N\\right)|_{a_i=\\mu_i\\left(o_i\\right)}\\right],$\n",
    "\n",
    "Here the experience replay buffer $\\mathcal{D}$ contains the tuples $\\left(x,a_1,...,a_N,r_1,...,r_N\\right)$,  recording\n",
    "experiences of all agents. The centralized action-value function $Q^\\mu_i$ is updated as:\n",
    "\n",
    "$\\mathcal{L}\\left(\\theta_i\\right) = \\mathbb{E}_{x,a,r,\\acute{x}}\\left[\\left(Q^\\mu_i\\left(x,a_1,...,a_N\\right)-y\\right)^2\\right],$, where \n",
    "$y = r_i + \\gamma Q^{\\acute{\\mu}}_i\\left(\\acute{x},\\acute{a_1},...,\\acute{a_N} \\right)|_{\\acute{a_j}=\\acute{\\mu_i}\\left(o_j\\right)^,}$\n",
    "\n",
    "where $\\acute{\\mu} = \\{\\mu_\\acute{\\theta_1},...,\\mu_\\acute{\\theta_N}\\}$ is the set of target policies with delayed parameters $\\acute{\\theta_i}$. We find the centralized critic with deterministic policies works very well in practice, and refer to it as _multi-agent deep deterministic policy gradient_ (MADDPG). We provide the description of the full algorithm in the Appendix.\n",
    "\n",
    "A primary motivation behind MADDPG is that, if we know the actions taken by all agents, the\n",
    "environment is stationary even as the policies change, since $P\\left(\\acute{s}|s,a_1,...,a_N,\\pi_1,...,\\pi_N\\right) = P\\left(\\acute{s}|s,a_1,...,a_N \\right) = P\\left(\\acute{s}|s,a_1,...,a_N,\\acute{\\pi_1},...,\\acute{\\pi_N}\\right)$ for any $\\pi_i \\neq \\acute{\\pi_i}$. This is not the case if we do not explicitly condition on the actions of other agents, as done for most traditional RL methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a64e2-e384-4422-b888-9311767021e7",
   "metadata": {},
   "source": [
    "#### 5.2. Inferring Policies of Other Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4f869b-0a9f-49b7-95d5-95c0db81f4de",
   "metadata": {},
   "source": [
    "To remove the assumption of knowing other agents $^,$’ policies each agent $i$ can additionally maintain an approximation $\\hat{\\mu}_{\\phi_i^j}$ (where $\\phi$ are the parameters of the approximation; henceforth $\\hat{\\mu}_i^j$) to the true policy of agent $j,\\mu_j$. This approximate policy is learned by maximizing the log probability of agent $j^,$s actions, with an entropy regularizer:\n",
    "\n",
    "$\\mathcal{L}\\left(\\phi_i^j\\right) = -\\mathbb{E}_{o_j,a_j}\\left[log\\hat{\\mu}_i^j\\left(a_j|o_j\\right)+\\lambda H\\hat{\\mu}_i^j\\right],$\n",
    "\n",
    "where $H$ is the entropy of the policy distribution. With the approximate policies,$y$ can be\n",
    "replaced by an approximate value $\\hat{y}$ calculated as follows:\n",
    "\n",
    "$\\hat{y}=r_i+\\gamma Q^\\hat{\\mu}_i\\left(x^\\prime,\\hat{\\mu}^{\\prime1}_i\\left(o_1\\right),...,\\mu^\\prime_i\\left(o_i\\right),...,\\hat{\\mu}^{\\prime N}_i\\left(o_N\\right)\\right),$\n",
    "\n",
    "where $\\hat{\\mu}^{\\prime j}_i$ denotes the target network for the approximate policy$\\hat{\\mu}^{j}_i$  can be optimized in a completely online fashion: before updating $Q^\\mu_i$, the centralized $Q$ function, we take the latest samples of each agent $j$ from the replay buffer to perform a single gradient step to update $\\phi^j_i$. Note also that, in the above equation, we input the action log probabilities of each agent directly into $Q$, rather than sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a4fe1-c395-473f-8e4d-75ad065ba2e5",
   "metadata": {},
   "source": [
    "#### 5.3. Agents with Policy Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ecff6-0bdd-488f-87e9-d644fb1d37e5",
   "metadata": {},
   "source": [
    "As previously mentioned, a recurring problem in multi-agent reinforcement learning is the environment non-stationarity due to the agents’ changing policies. This is particularly true in competitive settings, where agents can derive a strong policy by overfitting to the behavior of their competitors.Such policies are undesirable as they are brittle and may fail when the competitors alter strategies.\n",
    "\n",
    "To obtain multi-agent policies that are more robust to changes in the policy of competing agents, we propose to train a collection of $K$ different sub-policies. At each episode, we randomly select\n",
    "one particular sub-policy for each agent to execute. Suppose that policy $\\mu_i$ i\n",
    "is an ensemble of $K$ different sub-policies with sub-policy $k$ denoted by $\\mu_{\\theta^{\\left(k\\right)}_i}$ (denoted as $\\mu^\\left(k\\right)_i$). For agen $i$,we are then\n",
    "maximizing the ensemble objective: $J_e\\left(\\mu_i\\right) = \\mathbb{E}_{k\\thicksim unif\\left(1,K\\right),s\\thicksim p^\\mu,a\\thicksim \\mu^\\left(k\\right)_i}\\left[R_i\\left(s,a\\right)\\right]$\n",
    "\n",
    "Since different sub-policies will be executed in different episodes, we maintain a replay buffer $\\mathcal{D}^{\\left(k \\right)}_i$ for each sub-policy $\\mu^{\\left(k \\right)}_i$ of agent $i$ Accordingly, we can derive the gradient of the ensemble objective\n",
    "with respect to $\\theta^{\\left(k \\right)}_i$ as follows:\n",
    "\n",
    "$\\nabla_\\theta{^{\\left(k \\right)}_i}J_e\\left(\\mu_i\\right)=\\frac{1}{K}\\mathbb{E}_{x,a \\thicksim}\\mathcal{D}^{\\left(k \\right)}_i\\left[\\nabla_\\theta{^{\\left(k \\right)}_i} \\mu^{\\left(k \\right)}_i\\left(a_i|o_i\\right)\\nabla_{a_i}Q^{\\mu_i}\\left(x,a_1,...,a_N\\right)|_{a_i=\\mu^{\\left(k \\right)}_i}\\left(o_i\\right)\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ddeb0b-c5bd-4881-bfba-cceb014934ab",
   "metadata": {},
   "source": [
    "### 6. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a2a4d-a551-4ae0-b067-666904741beb",
   "metadata": {},
   "source": [
    "#### 6.1. Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bbea13-7335-4f12-bf08-745741be227f",
   "metadata": {},
   "source": [
    "We accept the grounded communication medium, which consists of $N$ agents and $L$ landmarks inhabiting a two-dimensional world with a continuous\n",
    "space and discrete time. Agents can take physical actions in the environment and communication\n",
    "actions that are broadcast to other agents. we do not assume that all agents have\n",
    "identical spaces of action and observation, or act according to the same policy $\\pi$. We also consider\n",
    "games that are both cooperative (all agents must maximize total return) and competitive (agents\n",
    "have conflicting goals). Some environments require explicit communication between agents in a queue\n",
    "to achieve the best reward, while in other environments agents can only perform physical actions. We\n",
    "give details of each environment below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b53d2a-e85d-4d99-996d-f83dcd5b5d51",
   "metadata": {},
   "source": [
    "[Source: ](https://image.slidesharecdn.com/201208-maddpg-210423153651/85/multi-ppt-agent-actorcritic-for-mixed-cooperativecompetitive-environments-27-320.jpg?cb=1668205757)![](https://image.slidesharecdn.com/201208-maddpg-210423153651/85/multi-ppt-agent-actorcritic-for-mixed-cooperativecompetitive-environments-27-320.jpg?cb=1668205757)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bbddf2-98fe-442a-b043-c59f0bf07d07",
   "metadata": {},
   "source": [
    "__Cooperative communication.__ This task consists of two cooperative agents, a speaker and a listener, who are placed in an environment with three landmarks of differing colors. At each episode, the listener must navigate to a landmark of a particular color, and obtains reward based on its distance to the correct landmark. However, while the listener can observe the relative position and color of the landmarks, it does not know which landmark it must navigate to. Conversely, the speaker’s observation consists of the correct landmark color, and it can produce a communication output at each time step which is observed by the listener. Thus, the speaker must learn to output the landmark colour based on the motions of the listener.\n",
    "\n",
    "__Cooperative navigation.__ In this environment, agents must cooperate through physical actions to reach a set of L landmarks. Agents observe the relative positions of other agents and landmarks, and are collectively rewarded based on the proximity of any agent to each landmark. In other words, the agents have to ‘cover’ all of the landmarks. Further, the agents occupy significant physical space and are penalized when colliding with each other. Our agents learn to infer the landmark they must cover, and move there while avoiding other agents.\n",
    "\n",
    "__Keep-away.__ This scenario consists of $L$ landmarks including a target landmark, $N$ cooperating agents who know the target landmark and are rewarded based on their distance to the target, and $M$ adversarial agents who must prevent the cooperating agents from reaching the target. Adversaries accomplish this by physically pushing the agents away from the landmark, temporarily occupying it. While the adversaries are also rewarded based on their distance to the target landmark, they do not know the correct target; this must be inferred from the movements of the agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1813e0-7b22-4396-9b32-dfb6e3f1e86d",
   "metadata": {},
   "source": [
    "[Source: ](https://image.slidesharecdn.com/201208-maddpg-210423153651/85/multi-ppt-agent-actorcritic-for-mixed-cooperativecompetitive-environments-30-320.jpg?cb=1668205757)![](https://image.slidesharecdn.com/201208-maddpg-210423153651/85/multi-ppt-agent-actorcritic-for-mixed-cooperativecompetitive-environments-30-320.jpg?cb=1668205757)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16274633-5feb-40cd-b1b7-dac8469d709e",
   "metadata": {},
   "source": [
    "__Physical deception.__ Here, $N$ agents cooperate to reach a single target landmark from a total of $N$ landmarks. They are rewarded based on the minimum distance of any agent to the target (so only one agent needs to reach the target landmark). However, a lone adversary also desires to reach the target landmark; the catch is that the adversary does not know which of the landmarks is the correct one. Thus the cooperating agents, who are penalized based on the adversary distance to the target, learn to spread out and cover all landmarks so as to deceive the adversary.\n",
    "\n",
    "__Predator-prey.__ In this variant of the classic predator-prey game, N slower cooperating agents must chase the faster adversary around a randomly generated environment with L large landmarks impeding the way. Each time the cooperative agents collide with an adversary, the agents are rewarded while the adversary is penalized. Agents observe the relative positions and velocities of the agents, and the positions of the landmarks.\n",
    "\n",
    "__Covert communication.__ This is an adversarial communication environment, where a speaker agent (‘Alice’) must communicate a message to a listener agent (‘Bob’), who must reconstruct the message at the other end. However, an adversarial agent (‘Eve’) is also observing the channel, and wants to reconstruct the message — Alice and Bob are penalized based on Eve’s reconstruction, and thus Alice must encode her message using a randomly generated key, known only to Alice and Bob. This is similar to the cryptography environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e67413-d150-48bd-a255-2e1735aedea7",
   "metadata": {},
   "source": [
    "#### 6.2. Comparison to Decentralized Reinforcement Learning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa0d9ed-41e3-49ee-b7e3-e2d451adf2b7",
   "metadata": {},
   "source": [
    "We implement our MADDPG algorithm and\n",
    "evaluate it on the environments presented.Unless otherwise specified, our\n",
    "policies are parameterized by a two-layer ReLU\n",
    "MLP with 64 units per layer. The messages\n",
    "sent between agents are soft approximations to\n",
    "discrete messages, calculated using the GumbelSoftmax estimator. To evaluate the quality\n",
    "of policies learned in competitive settings, we\n",
    "pitch MADDPG agents against DDPG agents,\n",
    "and compare the resulting success of the agents\n",
    "and adversaries in the environment. We train\n",
    "our models until convergence, and then evaluate\n",
    "them by averaging various metrics for 1000 further iterations. We provide the tables and details\n",
    "of our results on all environments in the Appendix, and summarize them here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2053519-803f-4b68-ba48-a8dca1445f31",
   "metadata": {},
   "source": [
    "[Source: ](data:image/jpeg;base64,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)![](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c514698-4b2f-4874-a500-6148b68f68da",
   "metadata": {},
   "source": [
    "First examine the cooperative communication scenario. Despite the simplicity of the task (the\n",
    "speaker only needs to learn to output its observation), traditional RL methods such as DQN, Actor-Critic, a first-order implementation of TRPO, and DDPG all fail to learn the correct behaviour\n",
    "(measured by whether the listener is within a short distance from the target landmark). In practice we\n",
    "observed that the listener learns to ignore the speaker and simply moves to the middle of all observed\n",
    "landmarks. plot the learning curves over 25000 episodes for various approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1736a618-9f22-4fc5-8804-19cf09f13f81",
   "metadata": {},
   "source": [
    "hypothesize that a primary reason for the failure of traditional RL methods in this (and other)\n",
    "multi-agent settings is the lack of a consistent gradient signal. For example, if the speaker utters\n",
    "the correct symbol while the listener moves in the wrong direction, the speaker is penalized. This\n",
    "problem is exacerbated as the number of time steps grows: we observed that traditional policy\n",
    "gradient methods can learn when the objective of the listener is simply to reconstruct the observation\n",
    "of the speaker in a single time step, or if the initial positions of agents and landmarks are fixed and\n",
    "evenly distributed. This indicates that many of the multi-agent methods previously proposed for\n",
    "scenarios with short time horizons may not generalize to more complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae510e0-9026-4bd6-9a55-1b7b760409dc",
   "metadata": {},
   "source": [
    "Conversely, MADDPG agents can learn coordinated behaviour more easily via the centralized\n",
    "critic. In the cooperative communication environment, MADDPG is able to reliably learn\n",
    "the correct listener and speaker policies, and\n",
    "the listener is often (84.0% of the time) able to\n",
    "navigate to the target.\n",
    "\n",
    "A similar situation arises for the physical deception task: when the cooperating agents are\n",
    "trained with MADDPG, they are able to successfully deceive the adversary by covering all of the\n",
    "landmarks around 94% of the time when $L = 2$\n",
    "(Figure 5). Furthermore, the adversary success\n",
    "is quite low, especially when the adversary is\n",
    "trained with DDPG (16.4% when $L = 2$). This\n",
    "contrasts sharply with the behaviour learned by\n",
    "the cooperating DDPG agents, who are unable to deceive MADDPG adversaries in any scenario, and\n",
    "do not even deceive other DDPG agents when $L = 4$.\n",
    "\n",
    "While the cooperative navigation and predator-prey tasks have a less stark divide between success and\n",
    "failure, in both cases the MADDPG agents outperform the DDPG agents. In cooperative navigation,\n",
    "MADDPG agents have a slightly smaller average distance to each landmark, but have almost half the average number of collisions per episode (when $N = 2$) compared to DDPG agents due to the ease\n",
    "of coordination. Similarly, MADDPG predators are far more successful at chasing DDPG prey (16.1\n",
    "collisions/episode) than the converse (10.3 collisions/episode).\n",
    "\n",
    "In the covert communication environment, we found that Bob trained with both MADDPG and\n",
    "DDPG out-performs Eve in terms of reconstructing Alice’s message. However, Bob trained with\n",
    "MADDPG achieves a larger relative success rate compared with DDPG (52.4% to 25.1%). Further,\n",
    "only Alice trained with MADDPG can encode her message such that Eve achieves near-random\n",
    "reconstruction accuracy. The learning curve (a sample plot is shown in Appendix) shows that the\n",
    "oscillation due to the competitive nature of the environment often cannot be overcome with common\n",
    "decentralized RL methods. We emphasize that we do not use any of the tricks required for the\n",
    "cryptography environment, including modifying Eve’s loss function, alternating agent and\n",
    "adversary training, and using a hybrid ‘mix & transform’ feed-forward and convolutional architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ae703e-20fe-409e-a671-ce56cb6bbabb",
   "metadata": {},
   "source": [
    "#### 6.3. Effect of Learning Polices of Other Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17612b3e-b0ad-4872-9ca9-fdabdaad13d0",
   "metadata": {},
   "source": [
    "The authors evaluate the effectiveness of learning the policies of other agents in the cooperative communication\n",
    "environment, following the same hyperparameters as the previous experiments and setting $λ = 0.001$. They observe that despite not fitting the policies of other\n",
    "agents perfectly (in particular, the approximate listener policy learned by the speaker has a fairly\n",
    "large KL divergence to the true policy), learning with approximated policies is able to achieve the\n",
    "same success rate as using the true policy, without a significant slowdown in convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4fac8f-16fc-495f-a001-bc63cd117770",
   "metadata": {},
   "source": [
    "#### 6.4. Effect of Training with Policy Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4469ed-16ea-4b0f-bac6-7e3f469fd387",
   "metadata": {},
   "source": [
    "the authors focus on the effectiveness of policy ensembles in competitive environments, including keep-away,\n",
    "cooperative navigation, and predator-prey. They choose $K = 3$ sub-policies for the keep-away and\n",
    "cooperative navigation environments, and $K = 2$ for predator-prey. To improve convergence speed,\n",
    "They enforce that the cooperative agents should have the same policies at each episode, and similarly\n",
    "for the adversaries. To evaluate the approach, they measure the performance of ensemble policies\n",
    "and single policies in the roles of both agent and adversary. The results are shown on the right side. They observe that agents with policy ensembles are stronger than those with a single\n",
    "policy. In particular, when pitting ensemble agents against single policy adversaries (second to left\n",
    "bar cluster), the ensemble agents outperform the adversaries by a large margin compared to when the\n",
    "roles are reversed (third to left bar cluster)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca267c8e-f905-4bd5-ab97-1fdeae1669e7",
   "metadata": {},
   "source": [
    "### 7. Conclusions and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20cc42a-061f-4ebf-8027-53eb76462f02",
   "metadata": {},
   "source": [
    "The creators of this article have proposed a multi-agent policy gradient algorithm where agents learn a centralized critic\n",
    "based on the observations and actions of all agents. Empirically, our method outperforms traditional\n",
    "RL algorithms on a variety of cooperative and competitive multi-agent environments. They can further improve the performance of their method by training agents with an ensemble of policies, an approach\n",
    "they believe to be generally applicable to any multi-agent algorithm.\n",
    "\n",
    "One downside to their approach is that the input space of Q grows linearly (depending on what\n",
    "information is contained in x) with the number of agents N. This could be remedied in practice by,\n",
    "for example, having a modular Q function that only considers agents in a certain neighborhood of a\n",
    "given agent. They leave this investigation to future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789c95a-22f2-4763-aa64-3c41f7364219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "\n",
    "# Define your problem using python and openAI's gym API:\n",
    "class ParrotEnv(gym.Env):\n",
    "    \"\"\"Environment in which an agent must learn to repeat the seen observations.\n",
    "\n",
    "    Observations are float numbers indicating the to-be-repeated values,\n",
    "    e.g. -1.0, 5.1, or 3.2.\n",
    "\n",
    "    The action space is always the same as the observation space.\n",
    "\n",
    "    Rewards are r=-abs(observation - action), for all steps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        # Make the space (for actions and observations) configurable.\n",
    "        self.action_space = config.get(\n",
    "            \"parrot_shriek_range\", gym.spaces.Box(-1.0, 1.0, shape=(1, )))\n",
    "        # Since actions should repeat observations, their spaces must be the\n",
    "        # same.\n",
    "        self.observation_space = self.action_space\n",
    "        self.cur_obs = None\n",
    "        self.episode_len = 0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        \"\"\"Resets the episode and returns the initial observation of the new one.\n",
    "        \"\"\"\n",
    "        # Reset the episode len.\n",
    "        self.episode_len = 0\n",
    "        # Sample a random number from our observation space.\n",
    "        self.cur_obs = self.observation_space.sample()\n",
    "        # Return initial observation.\n",
    "        return self.cur_obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Takes a single step in the episode given `action`\n",
    "\n",
    "        Returns:\n",
    "            New observation, reward, done-flag, info-dict (empty).\n",
    "        \"\"\"\n",
    "        # Set `truncated` flag after 10 steps.\n",
    "        self.episode_len += 1\n",
    "        terminated = False\n",
    "        truncated = self.episode_len >= 10\n",
    "        # r = -abs(obs - action)\n",
    "        reward = -sum(abs(self.cur_obs - action))\n",
    "        # Set a new observation (random sample).\n",
    "        self.cur_obs = self.observation_space.sample()\n",
    "        return self.cur_obs, reward, terminated, truncated, {}\n",
    "\n",
    "\n",
    "# Create an RLlib Algorithm instance from a PPOConfig to learn how to\n",
    "# act in the above environment.\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        env=ParrotEnv,\n",
    "        # Config dict to be passed to our custom env's constructor.\n",
    "        env_config={\n",
    "            \"parrot_shriek_range\": gym.spaces.Box(-5.0, 5.0, (1, ))\n",
    "        },\n",
    "    )\n",
    "    # Parallelize environment rollouts.\n",
    "    .rollouts(num_rollout_workers=3)\n",
    ")\n",
    "# Use the config's `build()` method to construct a PPO object.\n",
    "algo = config.build()\n",
    "\n",
    "# Train for n iterations and report results (mean episode rewards).\n",
    "# Since we have to guess 10 times and the optimal reward is 0.0\n",
    "# (exact match between observation and action value),\n",
    "# we can expect to reach an optimal episode reward of 0.0.\n",
    "for i in range(5):\n",
    "    results = algo.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8299df76-1b4e-4b8f-ad53-f4eb74f53620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference (action computations) based on given env observations.\n",
    "# Note that we are using a slightly simpler env here (-3.0 to 3.0, instead\n",
    "# of -5.0 to 5.0!), however, this should still work as the agent has\n",
    "# (hopefully) learned to \"just always repeat the observation!\".\n",
    "env = ParrotEnv({\"parrot_shriek_range\": gym.spaces.Box(-3.0, 3.0, (1, ))})\n",
    "# Get the initial observation (some value between -10.0 and 10.0).\n",
    "obs, info = env.reset()\n",
    "terminated = truncated = False\n",
    "total_reward = 0.0\n",
    "# Play one episode.\n",
    "while not terminated and not truncated:\n",
    "    # Compute a single action, given the current observation\n",
    "    # from the environment.\n",
    "    action = algo.compute_single_action(obs)\n",
    "    # Apply the computed action in the environment.\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    # Sum up rewards for reporting purposes.\n",
    "    total_reward += reward\n",
    "# Report results.\n",
    "print(f\"Shreaked for 1 episode; total-reward={total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc9efb-89b8-49b9-b0f7-e1d30469c1c6",
   "metadata": {},
   "source": [
    "### 8. Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd16d3a4-452f-4e05-84fd-827ac8ebca15",
   "metadata": {},
   "source": [
    "[Article](https://arxiv.org/pdf/1706.02275v4.pdf)\n",
    "\n",
    "[Code](https://github.com/ray-project/ray/tree/master/rllib)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
